---
title: "Project-3-stacked"
author: "George Paul,"
date: "2023-03-29"
output: 
  prettydoc::html_pretty:
    theme: cayman
---

# Problem Statement
We are assuming the role of a consulting firm, hired by the company IBM to analyze their attrition data. IBM executives would like to be able to predict which employees will choose to leave the company, ideally for two reasons: retaining high-performing employees, and letting low-performing employees quit rather than offering a severance package. Our task is to build this model, and IBM has provided us with data on employees, both employees that have left the company and employees that have not left. We are given information on employees' personal lives, such as age, gender, relationship status, as well as information about their job satisfaction. As for job satisfaction, we have information on pay, raises, as well as survey responses about feelings about job involvement and job/environment satisfaction. We will proceed by fitting several models and predicting onto a test set, and using these outputs as predictors for our final stacked models. Our success will be measured by analyzing the kappa value, to see if we truly can predict which employees will stay or leave.

# Libraries
```{r}
library(class)
library(janitor)
library(neuralnet)
library(caret)
library(kernlab)
library(C50)
library(randomForest)
```

# Combined Data Frame
Below we read in several csv files, which are the outputs of our first-levels fitted models. We will combine these predictions together to make our final data frame, and these predictions will now be used as input variables to make our final models.
```{r}
test.labels <- read.csv("test.labels.csv")
ann.preds <- read.csv("ann.preds.csv")
tree.preds <- read.csv("decision.tree.csv")
knn.preds <- read.csv("employees.knn.csv")
logreg.preds <- read.csv("logreg.preds.csv")
rm.preds <- read.csv("rm.preds.csv")
svm.preds <- read.csv("svm.preds.csv")

combined.df <- data.frame(Attrition = test.labels[,2],
                          ann = ann.preds[,2],
                          tree = tree.preds[,2],
                          knn = knn.preds[,2],
                          logreg = logreg.preds[,2],
                          rm = rm.preds[,2],
                          svm = svm.preds[,2])

combined.df$Attrition <- as.factor(combined.df$Attrition)
combined.df$tree <- as.factor(combined.df$tree)
combined.df$rm <- as.factor(combined.df$rm)
combined.df$svm <- as.factor(combined.df$svm)

combined.df <- clean_names(combined.df)
kappa.table <- read.csv("kappa.table.csv")
kappa.table$X <- NULL
```

# New Train/Test
```{r}
n2 <- nrow(combined.df)
set.seed(0)
idx2 <- sample(n2, n2*.7)

final.train <- combined.df[idx2,]
final.test <- combined.df[-idx2,]
```

# Fit Final Models

## Random Forest
```{r}
set.seed(1)
final.rf <- randomForest(attrition ~ ., data = final.train)
final.rf.preds <- predict(final.rf, final.test)
confusionMatrix(final.rf.preds, final.test$attrition)
final.rf.kappa <- confusionMatrix(final.rf.preds, final.test$attrition)$overall[2]
```

## Boosted Decision Tree
```{r}
ctrl <- trainControl(method = "cv", number = 10,
                     selectionFunction = "oneSE")

grid_tree <- expand.grid(.trials = c(1,5,10),
                         .model = "tree",
                         .winnow = "FALSE")
final.tree <- train(attrition ~ ., data = final.train, method = "C5.0",
           metric = "Kappa",
           trControl = ctrl,
           tuneGrid = grid_tree)
final.tree
final.tree.preds <- predict(final.tree, final.test)
confusionMatrix(final.tree.preds, final.test$attrition)
final.tree.kappa <- confusionMatrix(final.tree.preds, final.test$attrition)$overall[2]
```

# Graphs/Figures
```{r}
kappa.table[7,1] <- "Final RF"
kappa.table[7,2] <- final.rf.kappa
kappa.table[8,1] <- "Final Tree"
kappa.table[8,2] <- final.tree.kappa

barplot(kappa.table$kappa, names.arg = kappa.table$model, xlab = "model", ylab = "kappa")
```

# Summary of Results
The kappa values of our final models are not extremely high, both floating in the 0.3 to 0.4 range, which is notably similar to the kappa values of the first level model. This implies that our models are not extremely effective in predicting which employees will leave. 
However, given that the first level models generally had low kappa values, it is possible that this dataset is not particularly telling of trends in IBM's employee attrition. Perhaps the dataset is missing an underlying reason of why employees leave, or it could also be true that we cannot find the true relationships without more data. The dataset only contained the records of 1,470 employees, which is a relatively small amount of data to attempt to model with. Perhaps with more data, a more effective model could be built to satsify IBM's business need.

## Identification of Significant Predictors (TODO)
By looking at models that include some sort of variable importance, we can determine which variables are the most predictive for employee attrition. The first level models we ran that will best showcase the significant variables are logistic regression and random forest. For logistic regression, we will look at the p-values of variables (after backward step selection). For the random forest, we will look at the variable importance plot.
The logistic regression summary:
```{r, echo=F}
summary(readRDS("logreg.rds"))
```
And now the random forest variable importance plot:
```{r, echo=F}
varImpPlot(readRDS("rf.rds"))
```
The following variables seem to be significant in both models:
* Job Role (Specifically sales roles) (+)
* Distance From Home (+)
* Total Working Years (-)
* Age (-)
* Years at Company (-)
* Overtime Work (+)
* Job Involvement (-)
* Other less significant variables

Based on this information, we can start to create a narrative of the typical employee that chooses to leave the company. This employee more likely works in the sales department as a representative. This employee is younger, hasn't been in the workforce as long, and is new to the company. This employee is dissatisfied with his or her job involvement and commute into work, and perhaps they are asked to perform overtime work.
Notably, some of these predictors are intuitively correlated. Naturally, a younger employee has worked for fewer years and is newer to the company. Perhaps the sales department is generally younger, and maybe newer employees are required to work overtime. However, it is logical that a young newcomer to the company is less settled into their role, and perhaps this is why they may want to leave the company.

## Clustering Results
We ran the clustering algorithms in the dataset, and we found the optimal number of clusters was 3. We then created a final cluster assignment using k-means clustering with k = 3. Below shows the average attrition rate by cluster:
```{r, echo=F}
cluster_attrition <- read.csv("cluster_attrition.csv")
cluster_attrition$X <- NULL
cluster_attrition
```
There appears to be one cluster with significantly higher attrition rates. Based on looking at the centers of the 3 clusters, we found the following attributes to be true of the cluster with the highest attrition rate:
* Younger
* Fewer Years in Current Role
* Unmarried
* More sales representatives
* Fewer total working years
* Fewer Years at Company

While the results of clustering seemed promise at first, we are hesitant to use clusters as a prediction rule for the following two reasons:
* These variables are mostly correlated. Younger employees would naturally have had fewer total working years, fewer years at company, be unmarried, etc.
* These variables are largely similar to the variables our models used to make predictions. Using these clusters wouldn't add anything to the models we already built.

# Estimating Financial Impact
Once again, here is the final confusion matrix:
```{r, echo=F}
confusionMatrix(final.tree.preds, final.test$attrition)
```
We correctly predict 11 employees to leave, have 5 employees we predict to leave that do not, and we have 25 employees that leave which we did not predict to leave. In other words, 11 of the 16 employees we predict to leave actually leave, but we only predict 11 of the 36 employees that actually leave. We are facing a high false negative rate, and we should address the impact. Below are some facts/assumptions we will use:
* A typical severance package is 2-4 weeks per year worked, we will assume 3
* We will use the average salary and the average number of years at the company
* Median monthly salary at the company is 4919 and median number of years is 5
* We will assume that the true positives are employees that we would have fired anyway.
Below is the calculation of the amount of money saved by not paying severance:
```{r}
# Number of employees * number of years worked * 3 weeks per year worked * monthly salary * 12 months per year / 52 weeks per year
saved_money <- 11*5*3*4919*12/52
saved_money
```

This is a significant amount of money saved for the company, but we should also consider the opportunity cost of the employees we incorrectly predicted. This is a hard number to determine, but the breakeven cost is simple:
```{r}
# saved_money / number of employees incorrectly guessed (36)
breakeven <- saved_money/36
breakeven
```
As long as the opportunity cost is less that this amount, this method is profitable. Below is that amount as a percentage of the average monthly income:
```{r}
breakeven / (12*4919) * 100
```
If the opportunity cost of incorrectly predicting that an employee will stay or leave is roughly 8.8 percent of their salary or less, than our method will be profitable for the company.